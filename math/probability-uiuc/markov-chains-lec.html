<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="color-scheme" content="dark light" />
<style>
html {font-family: BlinkMacSystemFont, 'Segoe UI', Roboto, 'Open Sans', sans-serif;}
body {margin: 0; padding: 0 2.5em; line-height: 1.5; font-size: 1.1em;}
a:not(:hover) {text-decoration: none;}
@media (max-width: 820px) {
    body {padding: 0 1em;}
    ul {padding-left: 1em;}
    ol {padding-left: 1.2em;}
}

/* color scheme */
body {color: #202020;}
a {color: rgb(3, 102, 214);}
a:visited {color: rgb(100, 0, 150);}
@media (prefers-color-scheme: dark) {
    body {background: rgb(27, 30, 32); color: rgb(192, 192, 192);}
    a {color: rgb(65, 175, 235);}
    a:visited {color: rgb(150, 100, 230);}
    img {filter: brightness(.8) contrast(1.2);}
}
</style>
</head>
<body>
<h1>Markov Chains</h1>
<p>Summary of notes from lectures by Sheldon Jacobson
for the course IE 410 at UIUC in Fall 2021.</p>
<h2>Intro</h2>
<ul>
<li>Markov chain definition. Discrete vs continuous.</li>
<li>Transition function and transition matrix. Denoted by P. P(i,j) = Pr(go from state i to state j).</li>
<li>Theorem: Each row of P sums to 1.</li>
<li>Embedded markov chain:<ul>
<li>Definition (markov chain that is obtained by eliminating self-loops).</li>
<li>Computing from P.</li>
</ul>
</li>
</ul>
<h2>Chapman-Kolmogorov Equations</h2>
<ul>
<li>Definition of m-step transition function and matrix.</li>
<li>Chapman-Kolmogorov equations for transition function.</li>
<li>Matrix form of Chapman-Kolmogorov equations.</li>
</ul>
<h2>Classification of States</h2>
<h3>Communicability and state classes</h3>
<ul>
<li>Definition: accessibility and communicability of states.</li>
<li>Theorem: communicability is an equivalence relation.</li>
<li>Definition: states classes and irreducibility.</li>
</ul>
<h3>Recurrence</h3>
<ul>
<li>Definition: recurrent state: P(eventually being re-visited) = 1. Non-recurrent state is called transient.</li>
<li>Theorem: If a state is recurrent, then it is visited infinitely often with probability 1.</li>
<li>Theorem: For a transient state, if P(eventual revisit) = f,
    then P(revisited exactly k times) = f^(k-1) * (1-f),
    so E(eventual revisits) = 1/(1-f).</li>
<li>Theorem: A state is recurrent iff expected number of revisits is infinite.</li>
<li>Theorem: E(eventual revisits) = <code>∑_{n≥0} P^n(i,i)</code>.</li>
<li>Theorem: Finite state markov chain has at least one recurrent state.</li>
<li>Theorem: recurrence is class property.</li>
<li>Example: random walk with infinite number of states and probability p of going right. Recurrence?</li>
<li>Theorem: <code>∏_(i≥0) (1-p_i) = 0 iff ∑_(i≥0) p_i = ∞</code>.</li>
<li>Example: <code>P(i, j) = {p_i if j=0, 1-p_i if j=i+1, 0 otherwise}</code>. Is 0 recurrent?</li>
</ul>
<h2>Infinitely often and almost everywhere</h2>
<ul>
<li>Definition: infinitely often.</li>
<li>Definition: almost everywhere.</li>
<li>Lemma: {A a.e.} ⊆ {A i.o}.</li>
<li>Lemma: not({A a.e.}) = {not(A) i.o}.</li>
<li>Borel-Cantelli Lemma (with proof): <code>∑_(i≥0) p_i</code> is finite implies P(A i.o.) = 0.</li>
<li>Theorem: <code>∑_(i≥0) p_i = ∞</code> implies P(A i.o) = 1 for ind events A.</li>
</ul>
<h2>Limiting probabilities</h2>
<ul>
<li>Periodicity of a state.</li>
<li>Definition: Positive (and null) recurrence.</li>
<li>Theorem (without proof): In a finite markov chain, all recurrent states are positive recurrent.</li>
<li>Definition: Ergodic: Positive recurrent and aperiodic.</li>
<li>Theorem (without proof): For an irreducible ergodic markov chain, limiting probabilities π exist
    and are unique solutions to 'π = πP and sum(π) = 1'.</li>
<li>Theorem (without proof): If π is the limiting probability distr, then the expected time between
    revisits to j is 1/π_j.</li>
<li>Theorem (without proof): For ergodic irreducible markov chains,
    rank(I-P) = n-1, where n is the number of states.</li>
<li>Example: <code>P(i, j) = {p_i if j=0, 1-p_i if j=i+1, 0 otherwise}</code>. Is 0 positive recurrent?<ul>
<li>Special cases: p_i = p, p_i = (i+1)/(i+2).</li>
</ul>
</li>
<li>Example: random walk with infinite number of states and probability p of going right. Positive Recurrence?</li>
<li>Example: expected lengths of uptimes and downtimes in breakdown-and-repair process.</li>
</ul>
<h2>Gambler's ruin problem</h2>
<h2>Mean time spent in transient states</h2>
<ul>
<li>Theorem: Let <code>P_T</code> be the submatrix of the transition matrix restricted to transient states.
    Let S be a matrix where S[i,j] = P(no. of visits to j | start at i). Then <code>S = (I-P_T)^(-1)</code>.</li>
<li>Theorem: Let f[i,j] = P(eventually reach j | start at i). Then f[i,j] = (S[i,j] - I[i,j])/S[j,j].</li>
</ul>
<h2>Branching Process</h2>
<ul>
<li>Definition. Let <code>X_n</code> be the number of people in the nth generation.</li>
<li>Compute <code>E(X_n|X_0)</code> and <code>Var(X_n|X_0)</code> in terms of <code>X_0</code>, <code>E(X_1|X_0)</code> and <code>Var(X_1|X_0)</code>.</li>
<li>Compute P(extinction).</li>
</ul>
<h2>Time-Reversible Markov chains</h2>
<ul>
<li>Theorem: Time-reverse of a markov chain is a markov chain.</li>
<li>Definition: Time-reversibility.</li>
<li>Theorem: <code>x_iP[i,j] = x_jP[j,i]</code> has <code>x = π</code> as the unique solution.</li>
<li>Theorem: A random walk (even with different transition probability for different states) is time-reversible.</li>
<li>Example: Stationary probabilities of a bounded random walk
    with different transition probabilities for different states.</li>
</ul>
<h2>Markov Decision Process</h2>
<ul>
<li>Definition</li>
<li>Theorem: Each stationary distribution corresponds to a policy and vice-versa.</li>
<li>Solving for the optimal stationary distribution using a linear program.</li>
</ul>
</body>
</html>
